{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n파이토치 쿠다설정\\nhttps://blog.naver.com/me_a_me/223570004477\\n\\npython = 3.11.11\\ntorch = 2.6.0\\ncuda = 12.4\\ncudnn = 9.1.0.70\\n\\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "파이토치 쿠다설정\n",
        "https://blog.naver.com/me_a_me/223570004477\n",
        "\n",
        "python = 3.11.11\n",
        "torch = 2.6.0\n",
        "cuda = 12.4\n",
        "cudnn = 9.1.0.70\n",
        "\n",
        "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59Y15zUTSM4j",
        "outputId": "ddef3d50-632e-4245-f978-afc445541d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
            "Path to dataset files: C:\\Users\\qwer\\.cache\\kagglehub\\datasets\\birdy654\\cifake-real-and-ai-generated-synthetic-images\\versions\\3\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jQx0uyfmStMH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import timm\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm  # Progress bars\n",
        "from PIL import ImageFile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import Subset, random_split\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CBLQtd_SvSM",
        "outputId": "3adebd04-9a3b-47b0-8e99-87b02bb0bc1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_REAL 이미지 파일 수: 50000\n",
            "test_REAL 이미지 파일 수: 10000\n",
            "train_FAKE 이미지 파일 수: 50000\n",
            "test_FAKE 이미지 파일 수: 10000\n",
            "이미지 파일 수: 120000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "dataset_base_path = path\n",
        "dataset_path_train_real = os.path.join(dataset_base_path, 'train/REAL')\n",
        "dataset_path_train_fake = os.path.join(dataset_base_path, 'train/FAKE')\n",
        "dataset_path_test_real = os.path.join(dataset_base_path, 'test/REAL')\n",
        "dataset_path_test_fake = os.path.join(dataset_base_path, 'test/FAKE')\n",
        "\n",
        "# 경로에서 이미지 파일 목록 가져오기\n",
        "image_files_train_real = [f for f in os.listdir(dataset_path_train_real) if f.endswith(('.jpg', '.png'))]\n",
        "image_files_test_real = [f for f in os.listdir(dataset_path_test_real) if f.endswith(('.jpg', '.png'))]\n",
        "image_files_train_fake = [f for f in os.listdir(dataset_path_train_fake) if f.endswith(('.jpg', '.png'))]\n",
        "image_files_test_fake = [f for f in os.listdir(dataset_path_test_fake) if f.endswith(('.jpg', '.png'))]\n",
        "\n",
        "print(f\"train_REAL 이미지 파일 수: {len(image_files_train_real)}\")\n",
        "print(f\"test_REAL 이미지 파일 수: {len(image_files_test_real)}\")\n",
        "print(f\"train_FAKE 이미지 파일 수: {len(image_files_train_fake)}\")\n",
        "print(f\"test_FAKE 이미지 파일 수: {len(image_files_test_fake)}\")\n",
        "\n",
        "# 목록 합치기\n",
        "image_files = image_files_train_real + image_files_train_fake + image_files_test_real + image_files_test_fake\n",
        "\n",
        "print(f\"이미지 파일 수: {len(image_files)}\")\n",
        "\n",
        "# 이후 코드는 image_files를 사용하여 진행\n",
        "# ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pC7R4mqXr6ex"
      },
      "outputs": [],
      "source": [
        "#Dataset transformations are specified here\n",
        "\n",
        "IMG_SIZE = 224  # Swin Transformer input size\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE), # 크기변경\n",
        "    A.HorizontalFlip(p=0.5), # 좌우 뒤집기\n",
        "    A.RandomBrightnessContrast(p=0.2), # 밝기와 대비 변경\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)), # 정규화\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "test_transform = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KexxE7kGr_QS"
      },
      "outputs": [],
      "source": [
        "#Custom dataloader\n",
        "class CustomDataset(ImageFolder):\n",
        "    def __init__(self, root, transform=None):\n",
        "        super().__init__(root, transform=None)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, label = self.samples[index]\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        image = np.array(image)\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z9qKrlgsFLS",
        "outputId": "d64545d8-e850-48a9-a6dd-690c87c77616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100000\n",
            "10000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "train_dataset = CustomDataset(root=os.path.join(dataset_base_path, 'train'), transform=train_transform)\n",
        "test_dataset = CustomDataset(root=os.path.join(dataset_base_path, 'test'), transform=test_transform)\n",
        "\n",
        "# test와 validation 데이터 분리\n",
        "fake_samples = [sample for sample in test_dataset.samples if sample[1] == test_dataset.class_to_idx['FAKE']]\n",
        "real_samples = [sample for sample in test_dataset.samples if sample[1] == test_dataset.class_to_idx['REAL']]\n",
        "# test dataset을 validation과 test로 분할 (각각 10000개) real 5000개 fake5000개\n",
        "real_validation_size = 5000\n",
        "fake_validation_size = 5000\n",
        "real_test_size = 5000\n",
        "fake_test_size  = 5000\n",
        "\n",
        "fake_validation_dataset, fake_test_dataset = random_split(fake_samples, [fake_validation_size, fake_test_size])\n",
        "real_validation_dataset, real_test_dataset = random_split(real_samples, [real_validation_size, real_test_size])\n",
        "\n",
        "# Subset을 사용하여 데이터셋 생성\n",
        "validation_samples = [fake_samples[i] for i in fake_validation_dataset.indices] + [real_samples[i] for i in real_validation_dataset.indices]\n",
        "test_samples = [fake_samples[i] for i in fake_test_dataset.indices] + [real_samples[i] for i in real_test_dataset.indices]\n",
        "\n",
        "validation_dataset = Subset(test_dataset, [test_dataset.samples.index(sample) for sample in validation_samples])\n",
        "test_dataset = Subset(test_dataset, [test_dataset.samples.index(sample) for sample in test_samples])\n",
        "\n",
        "# 빠른 테스트를 위해 1000개만 불러오기 (real과 fake데이터 비율 안맞아서 고쳐야됨)\n",
        "# indices = list(range(2000))\n",
        "# train_dataset = Subset(train_dataset, indices)\n",
        "# validation_dataset = Subset(validation_dataset, indices)\n",
        "# test_dataset = Subset(test_dataset, indices)\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "print(len(validation_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrc9UWxBuBd2",
        "outputId": "66f363a1-e95a-467d-ab33-acbc025d7bc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test 데이터셋 클래스별 개수: {'REAL': 5000, 'FAKE': 5000}\n",
            "Validation 데이터셋 클래스별 개수: {'REAL': 5000, 'FAKE': 5000}\n"
          ]
        }
      ],
      "source": [
        "# 클래스별 데이터 개수 확인\n",
        "test_labels = [sample[1] for sample in test_dataset]\n",
        "test_class_counts = Counter(test_labels)\n",
        "validation_labels = [sample[1] for sample in validation_dataset]\n",
        "validation_class_counts = Counter(validation_labels)\n",
        "\n",
        "# 클래스 이름으로 출력\n",
        "class_names = [\"REAL\", \"FAKE\"]#train_dataset.classes\n",
        "test_class_counts_by_name = {class_names[label]: count for label, count in test_class_counts.items()}\n",
        "print(f\"test 데이터셋 클래스별 개수: {test_class_counts_by_name}\")\n",
        "validation_class_counts_by_name = {class_names[label]: count for label, count in validation_class_counts.items()}\n",
        "print(f\"Validation 데이터셋 클래스별 개수: {validation_class_counts_by_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nUyjtztushgi"
      },
      "outputs": [],
      "source": [
        "#이미지 메모리 제한 해제\n",
        "Image.MAX_IMAGE_PIXELS = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUOvwfP3sn63",
        "outputId": "fc65625e-999a-4411-ddd8-100abe87b7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "class SwinClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SwinClassifier, self).__init__()\n",
        "        self.model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=num_classes)\n",
        "        # self.model.head = nn.Linear(self.model.head.in_features, num_classes)  # Adjust classifier\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# cuda check\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = SwinClassifier(num_classes=2).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6_4GnNYNtpgC"
      },
      "outputs": [],
      "source": [
        "#Defining loss function and optimiser\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d8bb831ac4aa497698a0ef2352643a67",
            "4acd902418e14337b572891c67911f16",
            "75afa36b7bdc48468c3d5474d6aaeb4e",
            "a8a700745e204885ae1ea0b1bf47fc60",
            "4e23253ac2e945179234a57c96f0bb39",
            "28e797390e094c7cabeee6e4cb8ff627",
            "9d23ea5cbbdf4d31a0f799b3b0db0df0",
            "0680bfe0f11143638f8b42fa475e91fb",
            "9ffa8b6ce8a7448bba1360896784a72e",
            "9368ab38055d48e5b5729520c272fcce",
            "a8eaca28642c45edba54feaa4018963d"
          ]
        },
        "id": "nrhblCUftttS",
        "outputId": "560f44a9-2c11-4019-8755-fa32792ea7a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint found. Resuming training from checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/20 - Train Loss: 0.6496, Train Acc: 0.6247 | Val Loss: 0.6320, Val Acc: 0.6384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/20 - Train Loss: 0.6447, Train Acc: 0.6306 | Val Loss: 0.6316, Val Acc: 0.6383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/20 - Train Loss: 0.6530, Train Acc: 0.6233 | Val Loss: 0.6364, Val Acc: 0.6378\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/20 - Train Loss: 0.6549, Train Acc: 0.6190 | Val Loss: 0.6364, Val Acc: 0.6366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/20 - Train Loss: 0.6613, Train Acc: 0.6137 | Val Loss: 0.6345, Val Acc: 0.6401\n",
            "Early stopping triggered. Training stopped.\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "ImageFile.LOAD_TRUNCATED_IMAGES = True  # Allow loading truncated images\n",
        "\n",
        "# Assume these variables are already defined:\n",
        "# - train_dataset: your training dataset\n",
        "# - test_dataset: your validation/test dataset\n",
        "# - SwinClassifier: your model definition\n",
        "\n",
        "# Set DataLoader parameters; using num_workers=0 for TPU stability\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "# Training hyperparameters\n",
        "EPOCHS = 10 #에포크 10\n",
        "PATIENCE = 3\n",
        "\n",
        "# Define your model, loss, optimizer, and scheduler\n",
        "model = SwinClassifier(num_classes=2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
        "\n",
        "# Checkpoint file path\n",
        "checkpoint_file = \"checkpoint.pth\"\n",
        "# checkpoint_file = \"/content/gdrive/MyDrive/best_swin_model.pth\"\n",
        "# Initialize or resume training variables\n",
        "start_epoch = 0\n",
        "if os.path.exists(checkpoint_file):\n",
        "    print(\"Checkpoint found. Resuming training from checkpoint...\")\n",
        "    # Load checkpoint to CPU first, then move state to TPU\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=torch.device(\"cpu\"))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "    epochs_no_improve = checkpoint['epochs_no_improve']\n",
        "    train_losses = checkpoint.get('train_losses', [])\n",
        "    val_losses = checkpoint.get('val_losses', [])\n",
        "    # Move the model to the TPU device\n",
        "    model.to(device)\n",
        "else:\n",
        "    best_val_loss = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    iteration = 0\n",
        "\n",
        "    # Training loop with a progress bar\n",
        "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Training\", leave=False)\n",
        "    for images, labels in train_pbar:\n",
        "        print(labels)\n",
        "        iteration += 1\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        # if outputs.dim() == 4:\n",
        "        #     outputs = outputs.flatten(1)\n",
        "        loss = criterion(outputs, labels.long())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # mark.step()  # Ensure TPU executes pending operations\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "        print(f\"Batch size: {batch_size}\")\n",
        "        train_loss += loss.item() * batch_size\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        print(predicted)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += batch_size\n",
        "\n",
        "        train_pbar.set_postfix({\n",
        "            \"Batch Loss\": f\"{loss.item():.4f}\",\n",
        "            \"Avg Loss\": f\"{train_loss/total:.4f}\",\n",
        "            \"Acc\": f\"{correct/total:.4f}\"\n",
        "        })\n",
        "\n",
        "    epoch_train_loss = train_loss / total if total > 0 else 0\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    train_acc = correct / total if total > 0 else 0\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            # if outputs.dim() == 4:\n",
        "            #     outputs = outputs.flatten(1)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "\n",
        "            batch_size = images.size(0)\n",
        "            val_loss += loss.item() * batch_size\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "            total_val += batch_size\n",
        "\n",
        "            val_pbar.set_postfix({\n",
        "                \"Batch Loss\": f\"{loss.item():.4f}\",\n",
        "                \"Avg Loss\": f\"{val_loss/total_val:.4f}\",\n",
        "                \"Acc\": f\"{correct_val/total_val:.4f}\"\n",
        "            })\n",
        "\n",
        "    epoch_val_loss = val_loss / total_val if total_val > 0 else 0\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    val_acc = correct_val / total_val if total_val > 0 else 0\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {epoch_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint after every epoch\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'epochs_no_improve': epochs_no_improve,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_file)\n",
        "    # Also save a separate model file for each epoch if desired\n",
        "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), \"best_swin_model.pth\")  # Save best model separately\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            print(\"Early stopping triggered. Training stopped.\")\n",
        "            break\n",
        "\n",
        "    scheduler.step()\n",
        "    # print(met.metrics_report())\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "32Clg5ZXv5f-"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"swin_transformer_real_fake.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                             "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.6305, Test Accuracy: 0.6406, F1 Score: 0.6241\n",
            "Confusion Matrix:\n",
            "[[3422 1578]\n",
            " [2016 2984]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from PIL import ImageFile\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Assume that test_dataset and your model class SwinClassifier are already defined\n",
        "BATCH_SIZE = 32\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "# checkpoint_path = \"swin_transformer_real_fake.pth\"\n",
        "checkpoint_path = \"best_swin_model.pth\"\n",
        "\n",
        "model = SwinClassifier(num_classes=2)  # Create an instance of your model\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(checkpoint)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define the loss function (if you want to compute test loss)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Run testing/inference\n",
        "correct = 0\n",
        "total = 0\n",
        "test_loss = 0.0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "test_pbar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # Flatten outputs if needed (if output shape is [N, C, H, W])\n",
        "        # if outputs.dim() == 4:\n",
        "        #     outputs = outputs.flatten(1)\n",
        "        loss = criterion(outputs, labels.long())\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "        test_loss += loss.item() * batch_size\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += batch_size\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        test_pbar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "avg_loss = test_loss / total if total > 0 else 0\n",
        "accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "# Calculate F1 score and confusion matrix\n",
        "f1 = f1_score(all_labels, all_preds, average='binary')  # or 'binary' for binary classification\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0680bfe0f11143638f8b42fa475e91fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28e797390e094c7cabeee6e4cb8ff627": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4acd902418e14337b572891c67911f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28e797390e094c7cabeee6e4cb8ff627",
            "placeholder": "​",
            "style": "IPY_MODEL_9d23ea5cbbdf4d31a0f799b3b0db0df0",
            "value": "Epoch 1/10 Training:  19%"
          }
        },
        "4e23253ac2e945179234a57c96f0bb39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75afa36b7bdc48468c3d5474d6aaeb4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0680bfe0f11143638f8b42fa475e91fb",
            "max": 3125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ffa8b6ce8a7448bba1360896784a72e",
            "value": 580
          }
        },
        "9368ab38055d48e5b5729520c272fcce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d23ea5cbbdf4d31a0f799b3b0db0df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ffa8b6ce8a7448bba1360896784a72e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8a700745e204885ae1ea0b1bf47fc60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9368ab38055d48e5b5729520c272fcce",
            "placeholder": "​",
            "style": "IPY_MODEL_a8eaca28642c45edba54feaa4018963d",
            "value": " 580/3125 [08:41&lt;23:00,  1.84it/s, Batch Loss=0.2049, Avg Loss=0.3730, Acc=0.8503]"
          }
        },
        "a8eaca28642c45edba54feaa4018963d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8bb831ac4aa497698a0ef2352643a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4acd902418e14337b572891c67911f16",
              "IPY_MODEL_75afa36b7bdc48468c3d5474d6aaeb4e",
              "IPY_MODEL_a8a700745e204885ae1ea0b1bf47fc60"
            ],
            "layout": "IPY_MODEL_4e23253ac2e945179234a57c96f0bb39"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
